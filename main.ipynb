{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ce4c8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43b725af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 files belonging to 2 classes.\n",
      "Found 20000 files belonging to 2 classes.\n",
      "Train batch images: (128, 32, 32, 3)\n",
      "Train batch labels: (128, 1)\n"
     ]
    }
   ],
   "source": [
    "# setting root folder\n",
    "DATA_ROOT = \"dataset\"\n",
    "\n",
    "# common settings\n",
    "IMG_SIZE = (32, 32)\n",
    "BATCH_SIZE = 128\n",
    "SEED = 1\n",
    "\n",
    "# train dataset\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=os.path.join(DATA_ROOT, \"train\"),\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"binary\",\n",
    "    class_names=[\"FAKE\", \"REAL\"],  # ensure FAKE=0, REAL=1 (matches paper)\n",
    "    color_mode=\"rgb\",\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# test dataset\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory=os.path.join(DATA_ROOT, \"test\"),\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"binary\",\n",
    "    class_names=[\"FAKE\", \"REAL\"],  # same ordering as train\n",
    "    color_mode=\"rgb\",\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,                # keep test set order fixed\n",
    ")\n",
    "\n",
    "# normalization\n",
    "normalization_layer = tf.keras.layers.Rescaling(1.0 / 255.0)\n",
    "\n",
    "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "test_ds  = test_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "# data sanity check\n",
    "for images, labels in train_ds.take(1):\n",
    "    print(\"Train batch images:\", images.shape)\n",
    "    print(\"Train batch labels:\", labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82325bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train batches: 782\n",
      "Train batches after split: 704\n",
      "Val batches: 78\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# getting number of batches in the training dataset\n",
    "train_cardinality = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "val_ratio = 0.1\n",
    "val_batches = int(train_cardinality * val_ratio)\n",
    "print(\"Total train batches:\", train_cardinality)\n",
    "\n",
    "# creating validation and (reduced) training datasets\n",
    "val_ds = train_ds.take(val_batches)\n",
    "train_ds = train_ds.skip(val_batches)\n",
    "\n",
    "print(\"Train batches after split:\", tf.data.experimental.cardinality(train_ds).numpy())\n",
    "print(\"Val batches:\", tf.data.experimental.cardinality(val_ds).numpy())\n",
    "\n",
    "# adding cache + prefetching for performance\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds   = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds  = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86e3d9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cifake_cnn\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_image (InputLayer)    [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv1 (Conv2D)              (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " pool1 (MaxPooling2D)        (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2 (Conv2D)              (None, 16, 16, 32)        9248      \n",
      "                                                                 \n",
      " pool2 (MaxPooling2D)        (None, 8, 8, 32)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 64)                131136    \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 141345 (552.13 KB)\n",
      "Trainable params: 141345 (552.13 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model architecture\n",
    "# - 2 convolutional layers with 32 filters each (feature extractor)\n",
    "# - 1 dense hidden layer with 64 units\n",
    "# - 1 sigmoid output neuron for REAL (1) vs FAKE (0)\n",
    "\n",
    "def build_cifake_cnn(input_shape=(32, 32, 3)):\n",
    "    inputs = tf.keras.Input(shape=input_shape, name=\"input_image\")\n",
    "\n",
    "    # Conv block 1: 32 filters\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=(3, 3),\n",
    "        activation=\"relu\",\n",
    "        padding=\"same\",\n",
    "        name=\"conv1\",\n",
    "    )(inputs)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), name=\"pool1\")(x)\n",
    "\n",
    "    # Conv block 2: 32 filters\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=(3, 3),\n",
    "        activation=\"relu\",\n",
    "        padding=\"same\",\n",
    "        name=\"conv2\",\n",
    "    )(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), name=\"pool2\")(x)\n",
    "\n",
    "    # Flatten + Dense(64) head\n",
    "    x = tf.keras.layers.Flatten(name=\"flatten\")(x)\n",
    "    x = tf.keras.layers.Dense(64, activation=\"relu\", name=\"dense_64\")(x)\n",
    "\n",
    "    # Binary output: REAL (1) vs FAKE (0)\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"output\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"cifake_cnn\")\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_cifake_cnn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cded4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "704/704 [==============================] - 20s 27ms/step - loss: 0.3501 - accuracy: 0.8432 - precision: 0.8372 - recall: 0.8526 - val_loss: 0.2564 - val_accuracy: 0.8940 - val_precision: 0.8604 - val_recall: 0.9381\n",
      "Epoch 2/30\n",
      "704/704 [==============================] - 16s 23ms/step - loss: 0.2267 - accuracy: 0.9081 - precision: 0.9039 - recall: 0.9136 - val_loss: 0.2026 - val_accuracy: 0.9176 - val_precision: 0.8976 - val_recall: 0.9407\n",
      "Epoch 3/30\n",
      "704/704 [==============================] - 17s 25ms/step - loss: 0.1889 - accuracy: 0.9252 - precision: 0.9223 - recall: 0.9288 - val_loss: 0.1975 - val_accuracy: 0.9188 - val_precision: 0.8979 - val_recall: 0.9431\n",
      "Epoch 4/30\n",
      "704/704 [==============================] - 16s 23ms/step - loss: 0.1726 - accuracy: 0.9316 - precision: 0.9295 - recall: 0.9343 - val_loss: 0.1819 - val_accuracy: 0.9271 - val_precision: 0.9207 - val_recall: 0.9330\n",
      "Epoch 5/30\n",
      "704/704 [==============================] - 20s 28ms/step - loss: 0.1586 - accuracy: 0.9379 - precision: 0.9360 - recall: 0.9403 - val_loss: 0.1684 - val_accuracy: 0.9329 - val_precision: 0.9239 - val_recall: 0.9419\n",
      "Epoch 6/30\n",
      "704/704 [==============================] - 19s 27ms/step - loss: 0.1458 - accuracy: 0.9435 - precision: 0.9423 - recall: 0.9450 - val_loss: 0.1624 - val_accuracy: 0.9366 - val_precision: 0.9310 - val_recall: 0.9417\n",
      "Epoch 7/30\n",
      "704/704 [==============================] - 18s 26ms/step - loss: 0.1358 - accuracy: 0.9477 - precision: 0.9469 - recall: 0.9488 - val_loss: 0.1671 - val_accuracy: 0.9359 - val_precision: 0.9322 - val_recall: 0.9387\n",
      "Epoch 8/30\n",
      "704/704 [==============================] - 19s 27ms/step - loss: 0.1290 - accuracy: 0.9508 - precision: 0.9504 - recall: 0.9513 - val_loss: 0.1510 - val_accuracy: 0.9426 - val_precision: 0.9385 - val_recall: 0.9460\n",
      "Epoch 9/30\n",
      "704/704 [==============================] - 19s 27ms/step - loss: 0.1208 - accuracy: 0.9545 - precision: 0.9542 - recall: 0.9548 - val_loss: 0.1443 - val_accuracy: 0.9455 - val_precision: 0.9403 - val_recall: 0.9502\n",
      "Epoch 10/30\n",
      "704/704 [==============================] - 21s 30ms/step - loss: 0.1153 - accuracy: 0.9563 - precision: 0.9559 - recall: 0.9568 - val_loss: 0.1452 - val_accuracy: 0.9447 - val_precision: 0.9295 - val_recall: 0.9611\n",
      "Epoch 11/30\n",
      "704/704 [==============================] - 24s 34ms/step - loss: 0.1087 - accuracy: 0.9587 - precision: 0.9584 - recall: 0.9592 - val_loss: 0.1490 - val_accuracy: 0.9442 - val_precision: 0.9278 - val_recall: 0.9621\n",
      "Epoch 12/30\n",
      "704/704 [==============================] - 22s 31ms/step - loss: 0.1028 - accuracy: 0.9608 - precision: 0.9602 - recall: 0.9615 - val_loss: 0.1499 - val_accuracy: 0.9438 - val_precision: 0.9286 - val_recall: 0.9603\n"
     ]
    }
   ],
   "source": [
    "# configuring model\n",
    "EPOCHS = 30\n",
    "PATIENCE = 3\n",
    "\n",
    "# compiling model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"binary_crossentropy\",              # paper: binary cross-entropy\n",
    "    metrics=[\n",
    "        \"accuracy\",\n",
    "        tf.keras.metrics.Precision(name=\"precision\"),\n",
    "        tf.keras.metrics.Recall(name=\"recall\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# training model\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=PATIENCE,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff8a2261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 7ms/step\n",
      "Precision: 0.9339\n",
      "Recall:    0.9450\n",
      "F1 score:          0.9394\n"
     ]
    }
   ],
   "source": [
    "# evaluating model\n",
    "test_results = model.evaluate(test_ds, verbose=0)\n",
    "\n",
    "# test set results\n",
    "# for name, value in zip(model.metrics_names, test_results):\n",
    "#     print(f\"{name}: {value:.4f}\")\n",
    "\n",
    "# computing F1 score\n",
    "y_true = np.concatenate([y.numpy().ravel() for _, y in test_ds], axis=0)\n",
    "y_prob = model.predict(test_ds).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(\"int32\")\n",
    "\n",
    "# displaying precision, recall, and F1 score\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall    = recall_score(y_true, y_pred)\n",
    "f1        = f1_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 score:          {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b10541b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label (0=FAKE, 1=REAL): 0\n",
      "Heatmap shape: (32, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6d/6rs61bc54r94r2hj35rhqd4r0000gn/T/ipykernel_20604/715765847.py:105: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(\"True label (0=FAKE, 1=REAL):\", int(label))\n"
     ]
    }
   ],
   "source": [
    "# Grad-CAM heatmap functions\n",
    "def make_gradcam_heatmap(img_batch, model, last_conv_layer_name=\"conv2\"):\n",
    "    \"\"\"\n",
    "    img_batch: a 4D tensor (1, 32, 32, 3), already normalized like training data.\n",
    "    model: trained Keras model.\n",
    "    last_conv_layer_name: name of the last Conv2D layer (here \"conv2\").\n",
    "    Returns: 2D heatmap (32, 32) with values in [0, 1].\n",
    "    \"\"\"\n",
    "\n",
    "    # Build a model that maps input -> (last conv layer output, model output)\n",
    "    last_conv_layer = model.get_layer(last_conv_layer_name)\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs],\n",
    "        [last_conv_layer.output, model.output],\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_batch)\n",
    "        # For binary classification, we consider the logit for class \"REAL\" (1)\n",
    "        # Since output is sigmoid, predictions[:, 0] is p(REAL)\n",
    "        class_channel = predictions[:, 0]\n",
    "\n",
    "    # Gradient of the class score w.r.t. feature maps\n",
    "    grads = tape.gradient(class_channel, conv_outputs)[0]       # (H, W, C)\n",
    "    conv_outputs = conv_outputs[0]                              # (H, W, C)\n",
    "\n",
    "    # Global average pooling of gradients over H, W -> importance weights Î±_k\n",
    "    weights = tf.reduce_mean(grads, axis=(0, 1))                # (C,)\n",
    "\n",
    "    # Weighted sum over channels\n",
    "    cam = tf.zeros(conv_outputs.shape[:2], dtype=tf.float32)    # (H, W)\n",
    "    for i, w in enumerate(weights):\n",
    "        cam += w * conv_outputs[:, :, i]\n",
    "\n",
    "    # ReLU to keep only positive contributions (as per Grad-CAM definition)\n",
    "    cam = tf.nn.relu(cam)\n",
    "\n",
    "    # Normalize to [0, 1]\n",
    "    cam_max = tf.reduce_max(cam)\n",
    "    cam = cam / (cam_max + 1e-8)\n",
    "\n",
    "    # Resize to input size (32x32)\n",
    "    cam = tf.image.resize(cam[..., tf.newaxis], (32, 32))[..., 0]\n",
    "\n",
    "    return cam.numpy()\n",
    "\n",
    "def overlay_heatmap_on_image(image, heatmap, alpha=0.4):\n",
    "    \"\"\"\n",
    "    image: (32, 32, 3) float32 in [0,1]\n",
    "    heatmap: (32, 32) float32 in [0,1]\n",
    "    alpha: blending factor for heatmap\n",
    "    returns: RGB image with heatmap overlay, uint8 [0,255]\n",
    "    \"\"\"\n",
    "    # Ensure image is in [0,1]\n",
    "    if image.max() > 1.0:\n",
    "        image = image / 255.0\n",
    "\n",
    "    # Use 'jet' colormap\n",
    "    cmap = plt.get_cmap(\"jet\")\n",
    "    heatmap_color = cmap(heatmap)[:, :, :3]  # drop alpha channel\n",
    "\n",
    "    # Blend heatmap with original image\n",
    "    overlay = heatmap_color * alpha + image * (1.0 - alpha)\n",
    "    overlay = np.clip(overlay, 0.0, 1.0)\n",
    "    overlay = (overlay * 255).astype(\"uint8\")\n",
    "    return overlay\n",
    "\n",
    "\n",
    "def show_gradcam_example(image, label, model, last_conv_layer_name=\"conv2\"):\n",
    "    \"\"\"\n",
    "    image: (32, 32, 3) single image tensor/array\n",
    "    label: scalar 0/1\n",
    "    \"\"\"\n",
    "    # Ensure batch dimension\n",
    "    img_batch = image[None, ...]\n",
    "    heatmap = make_gradcam_heatmap(img_batch, model, last_conv_layer_name)\n",
    "\n",
    "    overlay = overlay_heatmap_on_image(image.numpy() if hasattr(image, \"numpy\") else image,\n",
    "                                       heatmap)\n",
    "\n",
    "    plt.figure(figsize=(6, 3))\n",
    "\n",
    "    # Original\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(f\"Original (label={int(label)})\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image)\n",
    "\n",
    "    # Overlay\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Grad-CAM overlay\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(overlay)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# generating Grad-CAM for one test image\n",
    "\n",
    "for imgs, labels in test_ds.take(1):\n",
    "    show_gradcam_example(imgs[0], labels[0], model, last_conv_layer_name=\"conv2\")\n",
    "    break\n",
    "\n",
    "heatmap = make_gradcam_heatmap(img, model, last_conv_layer_name=\"conv2\")\n",
    "print(\"True label (0=FAKE, 1=REAL):\", int(label))\n",
    "print(\"Heatmap shape:\", heatmap.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db59ca15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6d/6rs61bc54r94r2hj35rhqd4r0000gn/T/ipykernel_20604/4047923763.py:26: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  label = int(labels[i].numpy())  # 0=FAKE, 1=REAL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 20000 Grad-CAM overlays to 'gradcam_test'.\n"
     ]
    }
   ],
   "source": [
    "OUTPUT = \"gradcam_test\"\n",
    "\n",
    "# subfolders for REAL (1) and FAKE (0)\n",
    "os.makedirs(os.path.join(OUTPUT, \"REAL\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT, \"FAKE\"), exist_ok=True)\n",
    "\n",
    "def save_gradcam_overlays(dataset,\n",
    "                          model,\n",
    "                          last_conv_layer_name=\"conv2\",\n",
    "                          output=OUTPUT,\n",
    "                          max_images=None):\n",
    "    \"\"\"\n",
    "    dataset: tf.data.Dataset (e.g., test_ds)\n",
    "    model: trained model\n",
    "    last_conv_layer_name: last conv layer used for Grad-CAM\n",
    "    output: root directory for saving images\n",
    "    max_images: stop after saving this many images (None -> all)\n",
    "    \"\"\"\n",
    "    saved = 0\n",
    "\n",
    "    for batch_idx, (imgs, labels) in enumerate(dataset):\n",
    "        batch_size = imgs.shape[0]\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            img = imgs[i]          # (32, 32, 3), already normalized [0,1]\n",
    "            label = int(labels[i].numpy())  # 0=FAKE, 1=REAL\n",
    "\n",
    "            # Prepare batch of size 1 for Grad-CAM\n",
    "            img_batch = img[None, ...]\n",
    "            heatmap = make_gradcam_heatmap(\n",
    "                img_batch, model, last_conv_layer_name\n",
    "            )\n",
    "\n",
    "            # Overlay heatmap on original image\n",
    "            overlay = overlay_heatmap_on_image(img.numpy(), heatmap)  # uint8\n",
    "\n",
    "            # Choose class folder\n",
    "            cls_name = \"REAL\" if label == 1 else \"FAKE\"\n",
    "            cls_dir = os.path.join(OUTPUT, cls_name)\n",
    "\n",
    "            # Construct filename: e.g. REAL_batch0003_img0017.png\n",
    "            filename = f\"{cls_name}_b{batch_idx:04d}_i{i:04d}.png\"\n",
    "            filepath = os.path.join(cls_dir, filename)\n",
    "\n",
    "            # Save overlay using PIL\n",
    "            Image.fromarray(overlay).save(filepath)\n",
    "\n",
    "            saved += 1\n",
    "            if (max_images is not None) and (saved >= max_images):\n",
    "                print(f\"Reached max_images={max_images}, stopping.\")\n",
    "                return\n",
    "\n",
    "    print(f\"Saved {saved} Grad-CAM overlays to '{OUTPUT}'.\")\n",
    "\n",
    "\n",
    "# Run on ALL test images (set max_images to a number if you want a subset)\n",
    "save_gradcam_overlays(\n",
    "    test_ds,\n",
    "    model,\n",
    "    last_conv_layer_name=\"conv2\",\n",
    "    output=OUTPUT,\n",
    "    max_images=None,    # e.g. 500 if you just want first 500\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
